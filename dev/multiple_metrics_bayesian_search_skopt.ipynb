{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7c8824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T15:24:21.287123Z",
     "start_time": "2022-04-14T15:24:21.265791Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent.parent))\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9a85a",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58184bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T15:24:23.165175Z",
     "start_time": "2022-04-14T15:24:22.494521Z"
    }
   },
   "outputs": [],
   "source": [
    "# random search forecaster\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from skforecast.model_selection.model_selection import bayesian_search_forecaster\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360794f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T15:24:25.448031Z",
     "start_time": "2022-04-14T15:24:25.134042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dates      : 1991-07-01 00:00:00 --- 2001-01-01 00:00:00  (n=115)\n",
      "Validation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\n",
      "Test dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=29)\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv')\n",
    "data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n",
    "\n",
    "# Data preprocessing\n",
    "# ==============================================================================\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], format='%Y/%m/%d')\n",
    "data = data.set_index('datetime')\n",
    "data = data.asfreq('MS')\n",
    "data = data[['y']]\n",
    "data = data.sort_index()\n",
    "\n",
    "# Train-val-test dates\n",
    "# ==============================================================================\n",
    "end_train = '2001-01-01 23:59:00'\n",
    "end_val = '2006-01-01 23:59:00'\n",
    "\n",
    "print(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\n",
    "print(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\")\n",
    "print(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788937b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, RandomSampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) # disable optuna logs\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\n",
    "from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n",
    "from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79f061",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ceb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bayesian_search_skopt(\n",
    "    forecaster,\n",
    "    y: pd.Series,\n",
    "    search_space: dict,\n",
    "    steps: int,\n",
    "    metric: Union[str, callable, list],\n",
    "    initial_train_size: int,\n",
    "    fixed_train_size: bool=True,\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "    lags_grid: Optional[list]=None,\n",
    "    refit: bool=False,\n",
    "    n_trials: int=10,\n",
    "    random_state: int=123,\n",
    "    return_best: bool=True,\n",
    "    verbose: bool=True,\n",
    "    kwargs_gp_minimize: dict={}\n",
    ") -> Tuple[pd.DataFrame, object]:\n",
    "    \"\"\"\n",
    "    Bayesian optimization for a Forecaster object using time series backtesting and skopt library.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect, \n",
    "    ForecasterAutoregMultiOutput\n",
    "        Forcaster model.\n",
    "        \n",
    "    y : pandas Series\n",
    "        Training time series values. \n",
    "        \n",
    "    search_space : dict\n",
    "        Dictionary with parameters names (`str`) as keys and Space object from skopt \n",
    "        (Real, Integer, Categorical) as values.\n",
    "\n",
    "    steps : int\n",
    "        Number of steps to predict.\n",
    "        \n",
    "    metric : str, callable, list\n",
    "        Metric used to quantify the goodness of fit of the model.\n",
    "        \n",
    "        If string:\n",
    "            {'mean_squared_error', 'mean_absolute_error',\n",
    "             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n",
    "    \n",
    "        If callable:\n",
    "            Function with arguments y_true, y_pred that returns a float.\n",
    "\n",
    "        If list:\n",
    "            List containing several strings and/or callable.\n",
    "\n",
    "    initial_train_size : int \n",
    "        Number of samples in the initial train split.\n",
    " \n",
    "    fixed_train_size : bool, default `True`\n",
    "        If True, train size doesn't increases but moves by `steps` in each iteration.\n",
    "\n",
    "    exog : pandas Series, pandas DataFrame, default `None`\n",
    "        Exogenous variable/s included as predictor/s. Must have the same\n",
    "        number of observations as `y` and should be aligned so that y[i] is\n",
    "        regressed on exog[i].\n",
    "           \n",
    "    lags_grid : list of int, lists, np.narray or range, default `None`\n",
    "        Lists of `lags` to try. Only used if forecaster is an instance of \n",
    "        `ForecasterAutoreg`, `ForecasterAutoregDirect` or `ForecasterAutoregMultiOutput`.\n",
    "        \n",
    "    refit : bool, default `False`\n",
    "        Whether to re-fit the forecaster in each iteration of backtesting.\n",
    "        \n",
    "    n_trials : int, default `10`\n",
    "        Number of parameter settings that are sampled in each lag configuration.\n",
    "\n",
    "    random_state : int, default `123`\n",
    "        Sets a seed to the sampling for reproducible output.\n",
    "\n",
    "    return_best : bool, default `True`\n",
    "        Refit the `forecaster` using the best found parameters on the whole data.\n",
    "        \n",
    "    verbose : bool, default `True`\n",
    "        Print number of folds used for cv or backtesting.\n",
    "\n",
    "    kwargs_gp_minimize : dict, default `{}`\n",
    "        Other keyword arguments (key, value mappings) to pass to skopt.gp_minimize().\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        Results for each combination of parameters.\n",
    "            column lags = predictions.\n",
    "            column params = lower bound of the interval.\n",
    "            column metric = metric value estimated for the combination of parameters.\n",
    "            additional n columns with param = value.\n",
    "\n",
    "    results_opt_best : scipy object\n",
    "        The best optimization result returned as a OptimizeResult object.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(metric, list) and len(metric) != len(set(metric)):\n",
    "            raise ValueError(\n",
    "                'When `metrics` is a `list`, each metric name must be unique.'\n",
    "            )\n",
    "\n",
    "    if isinstance(forecaster, ForecasterAutoregCustom):\n",
    "        if lags_grid is not None:\n",
    "            warnings.warn(\n",
    "                '`lags_grid` ignored if forecaster is an instance of `ForecasterAutoregCustom`.'\n",
    "            )\n",
    "        lags_grid = ['custom predictors']\n",
    "        \n",
    "    elif lags_grid is None:\n",
    "        lags_grid = [forecaster.lags]\n",
    "   \n",
    "    lags_list = []\n",
    "    params_list = []\n",
    "    metric_list = []\n",
    "    results_opt_best = None\n",
    "\n",
    "    for key in search_space.keys():\n",
    "        if key != search_space[key].name:\n",
    "            raise Exception(\n",
    "                f\"\"\"Some of the key values do not match the Space object name from skopt.\n",
    "                    {key} != {search_space[key].name}.\"\"\"\n",
    "            )\n",
    "\n",
    "    search_space = list(search_space.values())\n",
    "    metric_values = [] # This variable will be modified inside _objective function. \n",
    "    # It is a trick to extract multiple values from _objective function since\n",
    "    # only the optimized value can be returned.\n",
    "\n",
    "    # Objective function using backtesting_forecaster\n",
    "    @use_named_args(search_space)\n",
    "    def _objective(\n",
    "        forecaster         = forecaster,\n",
    "        y                  = y,\n",
    "        exog               = exog,\n",
    "        initial_train_size = initial_train_size,\n",
    "        fixed_train_size   = fixed_train_size,\n",
    "        steps              = steps,\n",
    "        metric             = metric,\n",
    "        refit              = refit,\n",
    "        verbose            = verbose,\n",
    "        **params\n",
    "    ) -> float:\n",
    "        \n",
    "        forecaster.set_params(**params)\n",
    "        \n",
    "        metrics, _ = backtesting_forecaster(\n",
    "                        forecaster         = forecaster,\n",
    "                        y                  = y,\n",
    "                        exog               = exog,\n",
    "                        steps              = steps,\n",
    "                        metric             = metric,\n",
    "                        initial_train_size = initial_train_size,\n",
    "                        fixed_train_size   = fixed_train_size,\n",
    "                        refit              = refit,\n",
    "                        verbose            = verbose\n",
    "                    )\n",
    "        # Store metrics in the variable metrics_values defined outside _objective.\n",
    "        nonlocal metric_values\n",
    "        metric_values.append(metrics)\n",
    "\n",
    "        if isinstance(metrics, list):\n",
    "            return abs(metrics[0])\n",
    "        else:\n",
    "            return abs(metrics)\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Number of models compared: {n_trials*len(lags_grid)}, {n_trials} bayesian search in each lag configuration.\"\"\"\n",
    "    )\n",
    "\n",
    "    for lags in tqdm(lags_grid, desc='loop lags_grid', position=0, ncols=90):\n",
    "        \n",
    "        if isinstance(forecaster, (ForecasterAutoreg, ForecasterAutoregDirect, \n",
    "        ForecasterAutoregMultiOutput)):\n",
    "            forecaster.set_lags(lags)\n",
    "            lags = forecaster.lags.copy()\n",
    "        \n",
    "        results_opt = gp_minimize(\n",
    "                        func         = _objective,\n",
    "                        dimensions   = search_space,\n",
    "                        n_calls      = n_trials,\n",
    "                        random_state = random_state,\n",
    "                        **kwargs_gp_minimize\n",
    "                      )\n",
    "\n",
    "        for i in range(len(results_opt.x_iters)):\n",
    "            params = {param.name: results_opt.x_iters[i][j] \n",
    "                      for j, param in enumerate(search_space)}\n",
    " \n",
    "            params_list.append(params)\n",
    "            lags_list.append(lags)\n",
    "            if isinstance(metric, list):\n",
    "                for name, values in zip(metric, metric_values[i]):\n",
    "                    if not isinstance(name, str):\n",
    "                        name = name.__name__\n",
    "                    metric_list[name].append(values)\n",
    "            else:\n",
    "                metric_list.append(results_opt.func_vals[i])\n",
    "\n",
    "        if results_opt_best is None:\n",
    "            results_opt_best = results_opt\n",
    "        else:\n",
    "            if results_opt.fun < results_opt_best.fun:\n",
    "                results_opt_best = results_opt\n",
    "        \n",
    "    if isinstance(metric, list):\n",
    "        results = pd.DataFrame({\n",
    "                    'lags'  : lags_list,\n",
    "                    'params': params_list,\n",
    "                    **metric_list})\n",
    "        results = results.sort_values(by=list(metric_list)[0], ascending=True)\n",
    "    else:\n",
    "        results = pd.DataFrame({\n",
    "                    'lags'  : lags_list,\n",
    "                    'params': params_list,\n",
    "                    'metric': metric_list})\n",
    "        results = results.sort_values(by='metric', ascending=True)\n",
    "    \n",
    "    results = pd.concat([results, results['params'].apply(pd.Series)], axis=1)\n",
    "    \n",
    "    if return_best:\n",
    "        \n",
    "        best_lags = results['lags'].iloc[0]\n",
    "        best_params = results['params'].iloc[0]\n",
    "        best_metric = results['metric'].iloc[0]\n",
    "        \n",
    "        if isinstance(forecaster, (ForecasterAutoreg, ForecasterAutoregDirect, \n",
    "        ForecasterAutoregMultiOutput)):\n",
    "            forecaster.set_lags(best_lags)\n",
    "        forecaster.set_params(**best_params)\n",
    "        forecaster.fit(y=y, exog=exog)\n",
    "        \n",
    "        print(\n",
    "            f\"`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \\n\"\n",
    "            f\"  Lags: {best_lags} \\n\"\n",
    "            f\"  Parameters: {best_params}\\n\"\n",
    "            f\"  Backtesting metric: {best_metric}\\n\"\n",
    "        )\n",
    "\n",
    "    return results, results_opt_best\n",
    "\n",
    "\n",
    "def bayesian_search_forecaster(\n",
    "    forecaster,\n",
    "    y: pd.Series,\n",
    "    search_space: Union[callable, dict],\n",
    "    steps: int,\n",
    "    metric: Union[str, callable, list],\n",
    "    initial_train_size: int,\n",
    "    fixed_train_size: bool=True,\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "    lags_grid: Optional[list]=None,\n",
    "    refit: bool=False,\n",
    "    n_trials: int=10,\n",
    "    random_state: int=123,\n",
    "    return_best: bool=True,\n",
    "    verbose: bool=True,\n",
    "    engine: str='skopt',\n",
    "    kwargs_create_study: dict={},\n",
    "    kwargs_study_optimize: dict={},\n",
    "    kwargs_gp_minimize: dict={},\n",
    ") -> Tuple[pd.DataFrame, object]:\n",
    "    \"\"\"\n",
    "    Bayesian optimization for a Forecaster object using time series backtesting and \n",
    "    optuna or skopt library.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect,\n",
    "    ForecasterAutoregMultiOutput\n",
    "        Forcaster model.\n",
    "        \n",
    "    y : pandas Series\n",
    "        Training time series values. \n",
    "        \n",
    "    search_space : callable (optuna), dict (skopt)\n",
    "        If optuna engine: callable\n",
    "            Function with argument `trial` which returns a dictionary with parameters names \n",
    "            (`str`) as keys and Trial object from optuna (trial.suggest_float, \n",
    "            trial.suggest_int, trial.suggest_categorical) as values.\n",
    "\n",
    "        If skopt engine: dict\n",
    "            Dictionary with parameters names (`str`) as keys and Space object from skopt \n",
    "            (Real, Integer, Categorical) as values.\n",
    "\n",
    "    steps : int\n",
    "        Number of steps to predict.\n",
    "        \n",
    "    metric : str, callable, list\n",
    "        Metric used to quantify the goodness of fit of the model.\n",
    "        \n",
    "        If string:\n",
    "            {'mean_squared_error', 'mean_absolute_error',\n",
    "             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n",
    "    \n",
    "        If callable:\n",
    "            Function with arguments y_true, y_pred that returns a float.\n",
    "\n",
    "        If list:\n",
    "            List containing several strings and/or callable.\n",
    "\n",
    "\n",
    "    initial_train_size : int \n",
    "        Number of samples in the initial train split.\n",
    " \n",
    "    fixed_train_size : bool, default `True`\n",
    "        If True, train size doesn't increases but moves by `steps` in each iteration.\n",
    "\n",
    "    exog : pandas Series, pandas DataFrame, default `None`\n",
    "        Exogenous variable/s included as predictor/s. Must have the same\n",
    "        number of observations as `y` and should be aligned so that y[i] is\n",
    "        regressed on exog[i].\n",
    "           \n",
    "    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n",
    "        Lists of `lags` to try. Only used if forecaster is an instance of \n",
    "        `ForecasterAutoreg`, `ForecasterAutoregDirect` or `ForecasterAutoregMultiOutput`.\n",
    "        \n",
    "    refit : bool, default `False`\n",
    "        Whether to re-fit the forecaster in each iteration of backtesting.\n",
    "        \n",
    "    n_trials : int, default `10`\n",
    "        Number of parameter settings that are sampled in each lag configuration.\n",
    "\n",
    "    random_state : int, default `123`\n",
    "        Sets a seed to the sampling for reproducible output.\n",
    "\n",
    "    return_best : bool, default `True`\n",
    "        Refit the `forecaster` using the best found parameters on the whole data.\n",
    "        \n",
    "    verbose : bool, default `True`\n",
    "        Print number of folds used for cv or backtesting.\n",
    "\n",
    "    engine : str, default `'skopt'`\n",
    "        If 'optuna':\n",
    "            Bayesian optimization runs through the optuna library \n",
    "\n",
    "        If 'skopt':\n",
    "            Bayesian optimization runs through the skopt library\n",
    "\n",
    "    kwargs_create_study : dict, default `{'direction':'minimize', 'sampler':TPESampler(seed=123)}`\n",
    "        Only applies to engine='optuna'.\n",
    "            Keyword arguments (key, value mappings) to pass to optuna.create_study.\n",
    "\n",
    "    kwargs_study_optimize : dict, default `{}`\n",
    "        Only applies to engine='optuna'.\n",
    "            Other keyword arguments (key, value mappings) to pass to study.optimize().\n",
    "\n",
    "    kwargs_gp_minimize : dict, default `{}`\n",
    "        Only applies to engine='skopt'.\n",
    "            Other keyword arguments (key, value mappings) to pass to skopt.gp_minimize().\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        Results for each combination of parameters.\n",
    "            column lags = predictions.\n",
    "            column params = lower bound of the interval.\n",
    "            column metric = metric value estimated for the combination of parameters.\n",
    "            additional n columns with param = value.\n",
    "\n",
    "    results_opt_best : optuna object (optuna), scipy object (skopt)   \n",
    "        If optuna engine:\n",
    "            The best optimization result returned as a FrozenTrial optuna object.\n",
    "\n",
    "        If skopt engine:\n",
    "            The best optimization result returned as a OptimizeResult object.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if engine not in ['optuna', 'skopt']:\n",
    "        raise ValueError(\n",
    "                f\"\"\"`engine` only allows 'optuna' or 'skopt', got {engine}.\"\"\"\n",
    "              )\n",
    "\n",
    "    if engine == 'optuna':\n",
    "        results, results_opt_best = _bayesian_search_optuna(\n",
    "                                        forecaster            = forecaster,\n",
    "                                        y                     = y,\n",
    "                                        exog                  = exog,\n",
    "                                        lags_grid             = lags_grid,\n",
    "                                        search_space          = search_space,\n",
    "                                        steps                 = steps,\n",
    "                                        metric                = metric,\n",
    "                                        refit                 = refit,\n",
    "                                        initial_train_size    = initial_train_size,\n",
    "                                        fixed_train_size      = fixed_train_size,\n",
    "                                        n_trials              = n_trials,\n",
    "                                        random_state          = random_state,\n",
    "                                        return_best           = return_best,\n",
    "                                        verbose               = verbose,\n",
    "                                        kwargs_create_study   = kwargs_create_study,\n",
    "                                        kwargs_study_optimize = kwargs_study_optimize\n",
    "                                    )\n",
    "    else:\n",
    "        results, results_opt_best = _bayesian_search_skopt(\n",
    "                                        forecaster         = forecaster,\n",
    "                                        y                  = y,\n",
    "                                        exog               = exog,\n",
    "                                        lags_grid          = lags_grid,\n",
    "                                        search_space       = search_space,\n",
    "                                        steps              = steps,\n",
    "                                        metric             = metric,\n",
    "                                        refit              = refit,\n",
    "                                        initial_train_size = initial_train_size,\n",
    "                                        fixed_train_size   = fixed_train_size,\n",
    "                                        n_trials           = n_trials,\n",
    "                                        random_state       = random_state,\n",
    "                                        return_best        = return_best,\n",
    "                                        verbose            = verbose,\n",
    "                                        kwargs_gp_minimize = kwargs_gp_minimize\n",
    "                                    )\n",
    "\n",
    "    return results, results_opt_best\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d29533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 10,\n",
      "         5 bayesian search in each lag configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid:   0%|                                               | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.07048572681355265\n",
      "0.07048572681355265\n",
      "0.07091177923760796\n",
      "0.07112678795102378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid:  50%|███████████████████▌                   | 1/2 [00:00<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0711938276167963\n",
      "[0.07048572681355265, 0.07048572681355265, 0.07091177923760796, 0.07112678795102378, 0.0711938276167963]\n",
      "3\n",
      "0.06921754789783212\n",
      "0.06921754789783212\n",
      "0.06942956584984014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|███████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06907258765581369\n",
      "0.0689568868632526\n",
      "[0.07048572681355265, 0.07048572681355265, 0.07091177923760796, 0.07112678795102378, 0.0711938276167963, 0.06921754789783212, 0.06921754789783212, 0.06942956584984014, 0.06907258765581369, 0.0689568868632526]\n",
      "Number of models compared: 10,\n",
      "         5 bayesian search in each lag configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid:   0%|                                               | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0.07048572681355265]\n",
      "[0.07048572681355265]\n",
      "[0.07091177923760796]\n",
      "[0.07112678795102378]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid:  50%|███████████████████▌                   | 1/2 [00:00<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0711938276167963]\n",
      "[[0.07048572681355265], [0.07048572681355265], [0.07091177923760796], [0.07112678795102378], [0.0711938276167963]]\n",
      "3\n",
      "[0.06921754789783212]\n",
      "[0.06921754789783212]\n",
      "[0.06942956584984014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|███████████████████████████████████████| 2/2 [00:01<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06907258765581369]\n",
      "[0.0689568868632526]\n",
      "[[0.07048572681355265], [0.07048572681355265], [0.07091177923760796], [0.07112678795102378], [0.0711938276167963], [0.06921754789783212], [0.06921754789783212], [0.06942956584984014], [0.06907258765581369], [0.0689568868632526]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>12</td>\n",
       "      <td>1.258033</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.068957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>14</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.069073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>15</td>\n",
       "      <td>1.670328</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.069430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>17</td>\n",
       "      <td>1.454068</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.069218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>17</td>\n",
       "      <td>1.739441</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.069218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lags  n_estimators  min_samples_leaf max_features    metric\n",
       "5  [1 2 3]            12          1.258033         sqrt  0.068957\n",
       "6  [1 2 3]            14          1.081208         sqrt  0.069073\n",
       "7  [1 2 3]            15          1.670328         sqrt  0.069430\n",
       "8  [1 2 3]            17          1.454068         sqrt  0.069218\n",
       "9  [1 2 3]            17          1.739441         log2  0.069218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>12</td>\n",
       "      <td>1.258033</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.071194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>14</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.071127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>15</td>\n",
       "      <td>1.670328</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.070912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>17</td>\n",
       "      <td>1.454068</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.070486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>17</td>\n",
       "      <td>1.739441</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.070486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lags  n_estimators  min_samples_leaf max_features    metric\n",
       "5  [1 2 3]            12          1.258033         sqrt  0.071194\n",
       "6  [1 2 3]            14          1.081208         sqrt  0.071127\n",
       "7  [1 2 3]            15          1.670328         sqrt  0.070912\n",
       "8  [1 2 3]            17          1.454068         sqrt  0.070486\n",
       "9  [1 2 3]            17          1.739441         log2  0.070486"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bayesian search hyperparameter and lags with Optuna\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoreg(\n",
    "                regressor = RandomForestRegressor(random_state=123),\n",
    "                lags      = 10 # Placeholder, the value will be overwritten\n",
    "             )\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [5, 3]\n",
    "\n",
    "# Regressor hyperparameters search space\n",
    "def search_space(trial):\n",
    "    search_space  = {'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n",
    "                     'min_samples_leaf' : trial.suggest_float('min_samples_leaf', 1., 3.7, log=True),\n",
    "                     'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n",
    "                    } \n",
    "    return search_space\n",
    "\n",
    "results_1, frozen_trial = bayesian_search_forecaster(\n",
    "                            forecaster            = forecaster,\n",
    "                            y                     = data.loc[:end_val, 'y'],\n",
    "                            lags_grid             = lags_grid,\n",
    "                            search_space          = search_space,\n",
    "                            steps                 = 12,\n",
    "                            metric                = 'mean_squared_error',\n",
    "                            refit                 = True,\n",
    "                            initial_train_size    = len(data.loc[:end_train]),\n",
    "                            fixed_train_size      = True,\n",
    "                            n_trials              = 5,\n",
    "                            random_state          = 123,\n",
    "                            return_best           = False,\n",
    "                            verbose               = False,\n",
    "                            engine                = 'skopt',\n",
    "                            kwargs_create_study   = {},\n",
    "                            kwargs_study_optimize = {}\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "results_2, frozen_trial = bayesian_search_forecaster(\n",
    "                            forecaster            = forecaster,\n",
    "                            y                     = data.loc[:end_val, 'y'],\n",
    "                            lags_grid             = lags_grid,\n",
    "                            search_space          = search_space,\n",
    "                            steps                 = 12,\n",
    "                            metric                =  ['mean_squared_error'],\n",
    "                            refit                 = True,\n",
    "                            initial_train_size    = len(data.loc[:end_train]),\n",
    "                            fixed_train_size      = True,\n",
    "                            n_trials              = 5,\n",
    "                            random_state          = 123,\n",
    "                            return_best           = False,\n",
    "                            verbose               = False,\n",
    "                            engine                = 'skopt',\n",
    "                            kwargs_create_study   = {},\n",
    "                            kwargs_study_optimize = {}\n",
    "                        )\n",
    "\n",
    "\n",
    "results_1 = results_1.rename(columns={'mean_squared_error': 'metric'})\n",
    "results_2 = results_2.rename(columns={'mean_squared_error': 'metric'})\n",
    "\n",
    "results_1.lags = results_1.lags.astype(str)\n",
    "results_2.lags = results_2.lags.astype(str)\n",
    "\n",
    "cols = ['lags', 'n_estimators', 'min_samples_leaf', 'max_features', 'metric']\n",
    "cols_to_sort = ['lags', 'n_estimators', 'min_samples_leaf', 'max_features']\n",
    "\n",
    "results_1 = results_1[cols].sort_values(by=cols_to_sort).reset_index(drop=True)\n",
    "results_2 = results_2[cols].sort_values(by=cols_to_sort).reset_index(drop=True)\n",
    "\n",
    "equal_hiperparameters = (results_1[cols_to_sort] == results_2[cols_to_sort]).all(axis=1)\n",
    "\n",
    "results_1 = results_1[equal_hiperparameters]\n",
    "results_2 = results_2[equal_hiperparameters]\n",
    "\n",
    "\n",
    "no_match = results_1.metric != results_2.metric\n",
    "display(results_1[no_match])\n",
    "display(results_2[no_match])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01ee16",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ece7df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bayesian_search_optuna(\n",
    "    forecaster,\n",
    "    y: pd.Series,\n",
    "    search_space: callable,\n",
    "    steps: int,\n",
    "    metric: Union[str, callable, list],\n",
    "    initial_train_size: int,\n",
    "    fixed_train_size: bool=True,\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "    lags_grid: Optional[list]=None,\n",
    "    refit: bool=False,\n",
    "    n_trials: int=10,\n",
    "    random_state: int=123,\n",
    "    return_best: bool=True,\n",
    "    verbose: bool=True,\n",
    "    kwargs_create_study: dict={},\n",
    "    kwargs_study_optimize: dict={}\n",
    ") -> Tuple[pd.DataFrame, object]:\n",
    "    \"\"\"\n",
    "    Bayesian optimization for a Forecaster object using time series backtesting \n",
    "    and optuna library.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect,\n",
    "    ForecasterAutoregMultiOutput\n",
    "        Forcaster model.\n",
    "        \n",
    "    y : pandas Series\n",
    "        Training time series values. \n",
    "        \n",
    "    search_space : callable\n",
    "        Function with argument `trial` which returns a dictionary with parameters names \n",
    "        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n",
    "        trial.suggest_int, trial.suggest_categorical) as values.\n",
    "\n",
    "    steps : int\n",
    "        Number of steps to predict.\n",
    "        \n",
    "    metric : str, callable, list\n",
    "        Metric used to quantify the goodness of fit of the model.\n",
    "        \n",
    "        If string:\n",
    "            {'mean_squared_error', 'mean_absolute_error',\n",
    "             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n",
    "    \n",
    "        If callable:\n",
    "            Function with arguments y_true, y_pred that returns a float.\n",
    "\n",
    "        If list:\n",
    "            List containing several strings and/or callable.\n",
    "\n",
    "    initial_train_size : int \n",
    "        Number of samples in the initial train split.\n",
    " \n",
    "    fixed_train_size : bool, default `True`\n",
    "        If True, train size doesn't increases but moves by `steps` in each iteration.\n",
    "\n",
    "    exog : pandas Series, pandas DataFrame, default `None`\n",
    "        Exogenous variable/s included as predictor/s. Must have the same\n",
    "        number of observations as `y` and should be aligned so that y[i] is\n",
    "        regressed on exog[i].\n",
    "           \n",
    "    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n",
    "        Lists of `lags` to try. Only used if forecaster is an instance of \n",
    "        `ForecasterAutoreg`, `ForecasterAutoregDirect` or `ForecasterAutoregMultiOutput`.\n",
    "        \n",
    "    refit : bool, default `False`\n",
    "        Whether to re-fit the forecaster in each iteration of backtesting.\n",
    "        \n",
    "    n_trials : int, default `10`\n",
    "        Number of parameter settings that are sampled in each lag configuration.\n",
    "\n",
    "    random_state : int, default `123`\n",
    "        Sets a seed to the sampling for reproducible output.\n",
    "\n",
    "    return_best : bool, default `True`\n",
    "        Refit the `forecaster` using the best found parameters on the whole data.\n",
    "        \n",
    "    verbose : bool, default `True`\n",
    "        Print number of folds used for cv or backtesting.\n",
    "\n",
    "    kwargs_create_study : dict, default `{'direction':'minimize', 'sampler':TPESampler(seed=123)}`\n",
    "        Keyword arguments (key, value mappings) to pass to optuna.create_study.\n",
    "\n",
    "    kwargs_study_optimize : dict, default `{}`\n",
    "        Other keyword arguments (key, value mappings) to pass to study.optimize().\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        Results for each combination of parameters.\n",
    "            column lags = predictions.\n",
    "            column params = lower bound of the interval.\n",
    "            column metric = metric value estimated for the combination of parameters.\n",
    "            additional n columns with param = value.\n",
    "\n",
    "    results_opt_best : optuna object\n",
    "        The best optimization result returned as a FrozenTrial optuna object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(forecaster, ForecasterAutoregCustom):\n",
    "        if lags_grid is not None:\n",
    "            warnings.warn(\n",
    "                '`lags_grid` ignored if forecaster is an instance of `ForecasterAutoregCustom`.'\n",
    "            )\n",
    "        lags_grid = ['custom predictors']\n",
    "        \n",
    "    elif lags_grid is None:\n",
    "        lags_grid = [forecaster.lags]\n",
    "   \n",
    "    lags_list = []\n",
    "    params_list = []\n",
    "    results_opt_best = None\n",
    "    if not isinstance(metric, list):\n",
    "        metric = [metric] \n",
    "    metric_dict = {(m if isinstance(m, str) else m.__name__): [] for m in metric}\n",
    "    if len(metric_dict) != len(metric):\n",
    "        raise ValueError(\n",
    "            'When `metrics` is a `list`, each metric name must be unique.'\n",
    "        )\n",
    "\n",
    "    metrics_values = [] # This variable will be modified inside _objective function. \n",
    "    # It isa trick to extract multiple values from _objective function since\n",
    "    # only the optimized value can be returned.\n",
    "\n",
    "    # Objective function using backtesting_forecaster\n",
    "    def _objective(\n",
    "        trial,\n",
    "        forecaster         = forecaster,\n",
    "        y                  = y,\n",
    "        exog               = exog,\n",
    "        initial_train_size = initial_train_size,\n",
    "        fixed_train_size   = fixed_train_size,\n",
    "        steps              = steps,\n",
    "        metric             = metric,\n",
    "        refit              = refit,\n",
    "        verbose            = verbose,\n",
    "        search_space       = search_space,\n",
    "    ) -> float:\n",
    "        \n",
    "        forecaster.set_params(**search_space(trial))\n",
    "        \n",
    "        metrics, _ = backtesting_forecaster(\n",
    "                                forecaster         = forecaster,\n",
    "                                y                  = y,\n",
    "                                exog               = exog,\n",
    "                                steps              = steps,\n",
    "                                metric             = metric,\n",
    "                                initial_train_size = initial_train_size,\n",
    "                                fixed_train_size   = fixed_train_size,\n",
    "                                refit              = refit,\n",
    "                                verbose            = verbose\n",
    "                            )\n",
    "        # Store metrics in the variable metrics_values defined outside _objective.\n",
    "        nonlocal metrics_values\n",
    "        metrics_values.append(metrics)\n",
    "\n",
    "        return abs(metrics[0])\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Number of models compared: {n_trials*len(lags_grid)}, {n_trials} bayesian search in each lag configuration.\"\"\"\n",
    "    )\n",
    "\n",
    "    for lags in tqdm(lags_grid, desc='loop lags_grid', position=0, ncols=90):\n",
    "        \n",
    "        if isinstance(forecaster, (ForecasterAutoreg, ForecasterAutoregDirect, \n",
    "        ForecasterAutoregMultiOutput)):\n",
    "            forecaster.set_lags(lags)\n",
    "            lags = forecaster.lags.copy()\n",
    "        \n",
    "        if 'sampler' in kwargs_create_study.keys():\n",
    "            kwargs_create_study['sampler']._rng = np.random.RandomState(random_state)\n",
    "            kwargs_create_study['sampler']._random_sampler = RandomSampler(seed=random_state)    \n",
    "\n",
    "        study = optuna.create_study(**kwargs_create_study)\n",
    "\n",
    "        if 'sampler' not in kwargs_create_study.keys():\n",
    "            study.sampler = TPESampler(seed=random_state)\n",
    "\n",
    "        study.optimize(_objective, n_trials=n_trials, **kwargs_study_optimize)\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "\n",
    "        if search_space(best_trial).keys() != best_trial.params.keys():\n",
    "            raise Exception(\n",
    "                f\"\"\"Some of the key values do not match the search_space key names.\n",
    "                Dict keys     : {list(search_space(best_trial).keys())}\n",
    "                Trial objects : {list(best_trial.params.keys())}.\"\"\"\n",
    "            )\n",
    "\n",
    "        for i, trial in enumerate(study.get_trials()):\n",
    "            params_list.append(trial.params)\n",
    "            lags_list.append(lags)\n",
    "\n",
    "            for m, m_value in zip(metric, metrics_values[i]):\n",
    "                if isinstance(m, str):\n",
    "                    m_name = m\n",
    "                else:\n",
    "                    m_name = m.__name__\n",
    "                metric_dict[m_name].append(m_value)\n",
    "        \n",
    "        if results_opt_best is None:\n",
    "            results_opt_best = best_trial\n",
    "        else:\n",
    "            if best_trial.value < results_opt_best.value:\n",
    "                results_opt_best = best_trial\n",
    "        \n",
    "    results = pd.DataFrame({\n",
    "                'lags'  : lags_list,\n",
    "                'params': params_list,\n",
    "                **metric_dict})\n",
    "    results = results.sort_values(by=list(metric)[0], ascending=True)\n",
    "    results = pd.concat([results, results['params'].apply(pd.Series)], axis=1)\n",
    "    \n",
    "    if return_best:\n",
    "        \n",
    "        best_lags = results['lags'].iloc[0]\n",
    "        best_params = results['params'].iloc[0]\n",
    "        best_metric = results['metric'].iloc[0]\n",
    "        \n",
    "        if isinstance(forecaster, (ForecasterAutoreg, ForecasterAutoregDirect, \n",
    "        ForecasterAutoregMultiOutput)):\n",
    "            forecaster.set_lags(best_lags)\n",
    "        forecaster.set_params(**best_params)\n",
    "        forecaster.fit(y=y, exog=exog)\n",
    "        \n",
    "        print(\n",
    "            f\"`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \\n\"\n",
    "            f\"  Lags: {best_lags} \\n\"\n",
    "            f\"  Parameters: {best_params}\\n\"\n",
    "            f\"  Backtesting metric: {best_metric}\\n\"\n",
    "        )\n",
    "            \n",
    "    return results, results_opt_best\n",
    "\n",
    "\n",
    "def bayesian_search_forecaster(\n",
    "    forecaster,\n",
    "    y: pd.Series,\n",
    "    search_space: Union[callable, dict],\n",
    "    steps: int,\n",
    "    metric: Union[str, callable, list],\n",
    "    initial_train_size: int,\n",
    "    fixed_train_size: bool=True,\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "    lags_grid: Optional[list]=None,\n",
    "    refit: bool=False,\n",
    "    n_trials: int=10,\n",
    "    random_state: int=123,\n",
    "    return_best: bool=True,\n",
    "    verbose: bool=True,\n",
    "    engine: str='skopt',\n",
    "    kwargs_create_study: dict={},\n",
    "    kwargs_study_optimize: dict={},\n",
    "    kwargs_gp_minimize: dict={},\n",
    ") -> Tuple[pd.DataFrame, object]:\n",
    "    \"\"\"\n",
    "    Bayesian optimization for a Forecaster object using time series backtesting and \n",
    "    optuna or skopt library.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect,\n",
    "    ForecasterAutoregMultiOutput\n",
    "        Forcaster model.\n",
    "        \n",
    "    y : pandas Series\n",
    "        Training time series values. \n",
    "        \n",
    "    search_space : callable (optuna), dict (skopt)\n",
    "        If optuna engine: callable\n",
    "            Function with argument `trial` which returns a dictionary with parameters names \n",
    "            (`str`) as keys and Trial object from optuna (trial.suggest_float, \n",
    "            trial.suggest_int, trial.suggest_categorical) as values.\n",
    "\n",
    "        If skopt engine: dict\n",
    "            Dictionary with parameters names (`str`) as keys and Space object from skopt \n",
    "            (Real, Integer, Categorical) as values.\n",
    "\n",
    "    steps : int\n",
    "        Number of steps to predict.\n",
    "        \n",
    "    metric : str, callable, list\n",
    "        Metric used to quantify the goodness of fit of the model.\n",
    "        \n",
    "        If string:\n",
    "            {'mean_squared_error', 'mean_absolute_error',\n",
    "             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n",
    "    \n",
    "        If callable:\n",
    "            Function with arguments y_true, y_pred that returns a float.\n",
    "\n",
    "        If list:\n",
    "            List containing several strings and/or callable.\n",
    "\n",
    "\n",
    "    initial_train_size : int \n",
    "        Number of samples in the initial train split.\n",
    " \n",
    "    fixed_train_size : bool, default `True`\n",
    "        If True, train size doesn't increases but moves by `steps` in each iteration.\n",
    "\n",
    "    exog : pandas Series, pandas DataFrame, default `None`\n",
    "        Exogenous variable/s included as predictor/s. Must have the same\n",
    "        number of observations as `y` and should be aligned so that y[i] is\n",
    "        regressed on exog[i].\n",
    "           \n",
    "    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n",
    "        Lists of `lags` to try. Only used if forecaster is an instance of \n",
    "        `ForecasterAutoreg`, `ForecasterAutoregDirect` or `ForecasterAutoregMultiOutput`.\n",
    "        \n",
    "    refit : bool, default `False`\n",
    "        Whether to re-fit the forecaster in each iteration of backtesting.\n",
    "        \n",
    "    n_trials : int, default `10`\n",
    "        Number of parameter settings that are sampled in each lag configuration.\n",
    "\n",
    "    random_state : int, default `123`\n",
    "        Sets a seed to the sampling for reproducible output.\n",
    "\n",
    "    return_best : bool, default `True`\n",
    "        Refit the `forecaster` using the best found parameters on the whole data.\n",
    "        \n",
    "    verbose : bool, default `True`\n",
    "        Print number of folds used for cv or backtesting.\n",
    "\n",
    "    engine : str, default `'skopt'`\n",
    "        If 'optuna':\n",
    "            Bayesian optimization runs through the optuna library \n",
    "\n",
    "        If 'skopt':\n",
    "            Bayesian optimization runs through the skopt library\n",
    "\n",
    "    kwargs_create_study : dict, default `{'direction':'minimize', 'sampler':TPESampler(seed=123)}`\n",
    "        Only applies to engine='optuna'.\n",
    "            Keyword arguments (key, value mappings) to pass to optuna.create_study.\n",
    "\n",
    "    kwargs_study_optimize : dict, default `{}`\n",
    "        Only applies to engine='optuna'.\n",
    "            Other keyword arguments (key, value mappings) to pass to study.optimize().\n",
    "\n",
    "    kwargs_gp_minimize : dict, default `{}`\n",
    "        Only applies to engine='skopt'.\n",
    "            Other keyword arguments (key, value mappings) to pass to skopt.gp_minimize().\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        Results for each combination of parameters.\n",
    "            column lags = predictions.\n",
    "            column params = lower bound of the interval.\n",
    "            column metric = metric value estimated for the combination of parameters.\n",
    "            additional n columns with param = value.\n",
    "\n",
    "    results_opt_best : optuna object (optuna), scipy object (skopt)   \n",
    "        If optuna engine:\n",
    "            The best optimization result returned as a FrozenTrial optuna object.\n",
    "\n",
    "        If skopt engine:\n",
    "            The best optimization result returned as a OptimizeResult object.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if engine not in ['optuna', 'skopt']:\n",
    "        raise ValueError(\n",
    "                f\"\"\"`engine` only allows 'optuna' or 'skopt', got {engine}.\"\"\"\n",
    "              )\n",
    "\n",
    "    if engine == 'optuna':\n",
    "        results, results_opt_best = _bayesian_search_optuna(\n",
    "                                        forecaster            = forecaster,\n",
    "                                        y                     = y,\n",
    "                                        exog                  = exog,\n",
    "                                        lags_grid             = lags_grid,\n",
    "                                        search_space          = search_space,\n",
    "                                        steps                 = steps,\n",
    "                                        metric                = metric,\n",
    "                                        refit                 = refit,\n",
    "                                        initial_train_size    = initial_train_size,\n",
    "                                        fixed_train_size      = fixed_train_size,\n",
    "                                        n_trials              = n_trials,\n",
    "                                        random_state          = random_state,\n",
    "                                        return_best           = return_best,\n",
    "                                        verbose               = verbose,\n",
    "                                        kwargs_create_study   = kwargs_create_study,\n",
    "                                        kwargs_study_optimize = kwargs_study_optimize\n",
    "                                    )\n",
    "    else:\n",
    "        results, results_opt_best = _bayesian_search_skopt(\n",
    "                                        forecaster         = forecaster,\n",
    "                                        y                  = y,\n",
    "                                        exog               = exog,\n",
    "                                        lags_grid          = lags_grid,\n",
    "                                        search_space       = search_space,\n",
    "                                        steps              = steps,\n",
    "                                        metric             = metric,\n",
    "                                        refit              = refit,\n",
    "                                        initial_train_size = initial_train_size,\n",
    "                                        fixed_train_size   = fixed_train_size,\n",
    "                                        n_trials           = n_trials,\n",
    "                                        random_state       = random_state,\n",
    "                                        return_best        = return_best,\n",
    "                                        verbose            = verbose,\n",
    "                                        kwargs_gp_minimize = kwargs_gp_minimize\n",
    "                                    )\n",
    "\n",
    "    return results, results_opt_best\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dc9bc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 10, 5 bayesian search in each lag configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|███████████████████████████████████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 12, 'min_samples_leaf': 1.258...</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>12</td>\n",
       "      <td>1.258033</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 12, 'min_samples_leaf': 1.258...</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>12</td>\n",
       "      <td>1.258033</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 14, 'min_samples_leaf': 1.081...</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>14</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 14, 'min_samples_leaf': 1.081...</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>14</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.454...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.454068</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.739...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.739441</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.454...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.454068</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.739...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.739441</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 15, 'min_samples_leaf': 1.670...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>15</td>\n",
       "      <td>1.670328</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 15, 'min_samples_leaf': 1.670...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>15</td>\n",
       "      <td>1.670328</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lags                                             params  \\\n",
       "4        [1, 2, 3]  {'n_estimators': 12, 'min_samples_leaf': 1.258...   \n",
       "9  [1, 2, 3, 4, 5]  {'n_estimators': 12, 'min_samples_leaf': 1.258...   \n",
       "3        [1, 2, 3]  {'n_estimators': 14, 'min_samples_leaf': 1.081...   \n",
       "8  [1, 2, 3, 4, 5]  {'n_estimators': 14, 'min_samples_leaf': 1.081...   \n",
       "0        [1, 2, 3]  {'n_estimators': 17, 'min_samples_leaf': 1.454...   \n",
       "1        [1, 2, 3]  {'n_estimators': 17, 'min_samples_leaf': 1.739...   \n",
       "5  [1, 2, 3, 4, 5]  {'n_estimators': 17, 'min_samples_leaf': 1.454...   \n",
       "6  [1, 2, 3, 4, 5]  {'n_estimators': 17, 'min_samples_leaf': 1.739...   \n",
       "2        [1, 2, 3]  {'n_estimators': 15, 'min_samples_leaf': 1.670...   \n",
       "7  [1, 2, 3, 4, 5]  {'n_estimators': 15, 'min_samples_leaf': 1.670...   \n",
       "\n",
       "   mean_squared_error  n_estimators  min_samples_leaf max_features  \n",
       "4            0.068957            12          1.258033         sqrt  \n",
       "9            0.068957            12          1.258033         sqrt  \n",
       "3            0.069073            14          1.081208         sqrt  \n",
       "8            0.069073            14          1.081208         sqrt  \n",
       "0            0.069218            17          1.454068         sqrt  \n",
       "1            0.069218            17          1.739441         log2  \n",
       "5            0.069218            17          1.454068         sqrt  \n",
       "6            0.069218            17          1.739441         log2  \n",
       "2            0.069430            15          1.670328         sqrt  \n",
       "7            0.069430            15          1.670328         sqrt  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayesian search hyperparameter and lags with Optuna\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoreg(\n",
    "                regressor = RandomForestRegressor(random_state=123),\n",
    "                lags      = 10 # Placeholder, the value will be overwritten\n",
    "             )\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [3, 5]\n",
    "\n",
    "# Regressor hyperparameters search space\n",
    "def search_space(trial):\n",
    "    search_space  = {'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n",
    "                     'min_samples_leaf' : trial.suggest_float('min_samples_leaf', 1., 3.7, log=True),\n",
    "                     'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n",
    "                    } \n",
    "    return search_space\n",
    "\n",
    "results_1, frozen_trial = bayesian_search_forecaster(\n",
    "                            forecaster            = forecaster,\n",
    "                            y                     = data.loc[:end_val, 'y'],\n",
    "                            lags_grid             = lags_grid,\n",
    "                            search_space          = search_space,\n",
    "                            steps                 = 12,\n",
    "                            metric                = 'mean_squared_error',\n",
    "                            refit                 = True,\n",
    "                            initial_train_size    = len(data.loc[:end_train]),\n",
    "                            fixed_train_size      = True,\n",
    "                            n_trials              = 5,\n",
    "                            random_state          = 123,\n",
    "                            return_best           = False,\n",
    "                            verbose               = False,\n",
    "                            engine                = 'optuna',\n",
    "                            kwargs_create_study   = {},\n",
    "                            kwargs_study_optimize = {}\n",
    "                        )\n",
    "\n",
    "results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae39beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 10, 5 bayesian search in each lag configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop lags_grid: 100%|███████████████████████████████████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 12, 'min_samples_leaf': 1.258...</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>12</td>\n",
       "      <td>1.258033</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 12, 'min_samples_leaf': 1.258...</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>12</td>\n",
       "      <td>1.258033</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 14, 'min_samples_leaf': 1.081...</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>14</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 14, 'min_samples_leaf': 1.081...</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>14</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.454...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.454068</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.739...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.739441</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.454...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.454068</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 17, 'min_samples_leaf': 1.739...</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>17</td>\n",
       "      <td>1.739441</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'n_estimators': 15, 'min_samples_leaf': 1.670...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>15</td>\n",
       "      <td>1.670328</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'n_estimators': 15, 'min_samples_leaf': 1.670...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>15</td>\n",
       "      <td>1.670328</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lags                                             params  \\\n",
       "4        [1, 2, 3]  {'n_estimators': 12, 'min_samples_leaf': 1.258...   \n",
       "9  [1, 2, 3, 4, 5]  {'n_estimators': 12, 'min_samples_leaf': 1.258...   \n",
       "3        [1, 2, 3]  {'n_estimators': 14, 'min_samples_leaf': 1.081...   \n",
       "8  [1, 2, 3, 4, 5]  {'n_estimators': 14, 'min_samples_leaf': 1.081...   \n",
       "0        [1, 2, 3]  {'n_estimators': 17, 'min_samples_leaf': 1.454...   \n",
       "1        [1, 2, 3]  {'n_estimators': 17, 'min_samples_leaf': 1.739...   \n",
       "5  [1, 2, 3, 4, 5]  {'n_estimators': 17, 'min_samples_leaf': 1.454...   \n",
       "6  [1, 2, 3, 4, 5]  {'n_estimators': 17, 'min_samples_leaf': 1.739...   \n",
       "2        [1, 2, 3]  {'n_estimators': 15, 'min_samples_leaf': 1.670...   \n",
       "7  [1, 2, 3, 4, 5]  {'n_estimators': 15, 'min_samples_leaf': 1.670...   \n",
       "\n",
       "   mean_squared_error  n_estimators  min_samples_leaf max_features  \n",
       "4            0.068957            12          1.258033         sqrt  \n",
       "9            0.068957            12          1.258033         sqrt  \n",
       "3            0.069073            14          1.081208         sqrt  \n",
       "8            0.069073            14          1.081208         sqrt  \n",
       "0            0.069218            17          1.454068         sqrt  \n",
       "1            0.069218            17          1.739441         log2  \n",
       "5            0.069218            17          1.454068         sqrt  \n",
       "6            0.069218            17          1.739441         log2  \n",
       "2            0.069430            15          1.670328         sqrt  \n",
       "7            0.069430            15          1.670328         sqrt  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayesian search hyperparameter and lags with Optuna\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoreg(\n",
    "                regressor = RandomForestRegressor(random_state=123),\n",
    "                lags      = 10 # Placeholder, the value will be overwritten\n",
    "             )\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [3, 5]\n",
    "\n",
    "# Regressor hyperparameters search space\n",
    "def search_space(trial):\n",
    "    search_space  = {'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n",
    "                     'min_samples_leaf' : trial.suggest_float('min_samples_leaf', 1., 3.7, log=True),\n",
    "                     'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n",
    "                    } \n",
    "    return search_space\n",
    "\n",
    "results_2, frozen_trial = bayesian_search_forecaster(\n",
    "                            forecaster            = forecaster,\n",
    "                            y                     = data.loc[:end_val, 'y'],\n",
    "                            lags_grid             = lags_grid,\n",
    "                            search_space          = search_space,\n",
    "                            steps                 = 12,\n",
    "                            metric                = ['mean_squared_error'],\n",
    "                            refit                 = True,\n",
    "                            initial_train_size    = len(data.loc[:end_train]),\n",
    "                            fixed_train_size      = True,\n",
    "                            n_trials              = 5,\n",
    "                            random_state          = 123,\n",
    "                            return_best           = False,\n",
    "                            verbose               = False,\n",
    "                            engine                = 'optuna',\n",
    "                            kwargs_create_study   = {},\n",
    "                            kwargs_study_optimize = {}\n",
    "                        )\n",
    "\n",
    "results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34e72e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = results_1.rename(columns={'mean_squared_error': 'metric'})\n",
    "results_2 = results_2.rename(columns={'mean_squared_error': 'metric'})\n",
    "\n",
    "results_1.lags = results_1.lags.astype(str)\n",
    "results_2.lags = results_2.lags.astype(str)\n",
    "\n",
    "cols = ['lags', 'n_estimators', 'min_samples_leaf', 'max_features', 'metric']\n",
    "cols_to_sort = ['lags', 'n_estimators', 'min_samples_leaf', 'max_features']\n",
    "\n",
    "results_1 = results_1[cols].sort_values(by=cols_to_sort).reset_index(drop=True)\n",
    "results_2 = results_2[cols].sort_values(by=cols_to_sort).reset_index(drop=True)\n",
    "\n",
    "equal_hiperparameters = (results_1[cols_to_sort] == results_2[cols_to_sort]).all(axis=1)\n",
    "\n",
    "results_1 = results_1[equal_hiperparameters]\n",
    "results_2 = results_2[equal_hiperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6515111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [lags, n_estimators, min_samples_leaf, max_features, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [lags, n_estimators, min_samples_leaf, max_features, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "no_match = results_1.metric != results_2.metric\n",
    "display(results_1[no_match])\n",
    "display(results_2[no_match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477db131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
